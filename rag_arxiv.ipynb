{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import collections\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "import numpy as np\n",
    "\n",
    "import fitz  # PDF\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://arxiv.org/abs/2602.17654v1',\n",
       " 'title': 'Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval',\n",
       " 'abstract': 'We propose a two-stage \"Mine and Refine\" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.',\n",
       " 'published': '2026-02-19T18:56:36Z',\n",
       " 'authors': 'Jiaqi Xi, Raghav Saboo, Luming Chen, Martin Wang, Sudeep Das',\n",
       " 'link': 'https://arxiv.org/abs/2602.17654v1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARXIV_API = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "def fetch_arxiv(search_query: str, max_results: int = 50) -> List[Dict]:\n",
    "    params = {\n",
    "        \"search_query\": search_query,\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results,\n",
    "        \"sortBy\": \"submittedDate\",\n",
    "        \"sortOrder\": \"descending\",\n",
    "    }\n",
    "    r = requests.get(ARXIV_API, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    feed = feedparser.parse(r.text)\n",
    "    docs = []\n",
    "    for e in feed.entries:\n",
    "        doc = {\n",
    "            \"id\": e.get(\"id\"),\n",
    "            \"title\": (e.get(\"title\") or \"\").replace(\"\\n\", \" \").strip(),\n",
    "            \"abstract\": (e.get(\"summary\") or \"\").replace(\"\\n\", \" \").strip(),\n",
    "            \"published\": e.get(\"published\"),\n",
    "            \"authors\": \", \".join(a.name for a in e.get(\"authors\", [])),\n",
    "            \"link\": e.get(\"link\"),\n",
    "        }\n",
    "        docs.append(doc)\n",
    "\n",
    "   \n",
    "    time.sleep(2)\n",
    "    return docs\n",
    "\n",
    "# Example: ML Papers\n",
    "docs = fetch_arxiv(\"cat:cs.LG AND (all:retrieval OR all:RAG)\", max_results=50)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def abs_to_pdf_url(arxiv_abs_url: str) -> str:\n",
    "    # abs -> pdf\n",
    "    return arxiv_abs_url.replace(\"/abs/\", \"/pdf/\") + \".pdf\"\n",
    "\n",
    "def download_pdf_bytes(pdf_url: str) -> bytes:\n",
    "    r = requests.get(pdf_url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def pdf_to_text(pdf_bytes: bytes, max_pages: int = 12) -> str:\n",
    "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    texts = []\n",
    "    n_pages = min(len(doc), max_pages) #cap pages for now\n",
    "    for i in range(n_pages):\n",
    "        page = doc.load_page(i)\n",
    "        texts.append(page.get_text(\"text\"))\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    # remove excessive whitespace + hyphenated line breaks\n",
    "    text = re.sub(r\"-\\n\", \"\", text)          \n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)   \n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)   \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_references(text: str) -> str:\n",
    "    # Common section headers\n",
    "    m = re.search(r\"\\n(References|REFERENCES|Bibliography)\\n\", text)\n",
    "    if m:\n",
    "        return text[:m.start()].strip()\n",
    "    return text\n",
    "\n",
    "def build_fulltext_for_paper(d: Dict, max_pages: int = 12) -> Tuple[str, Dict]:\n",
    "    \"\"\"\n",
    "    Returns (full_text, extra_meta). If PDF fails, returns abstract-only text.\n",
    "    \"\"\"\n",
    "    base = (\n",
    "        f\"TITLE: {d['title']}\\n\"\n",
    "        f\"AUTHORS: {d['authors']}\\n\"\n",
    "        f\"PUBLISHED: {d['published']}\\n\"\n",
    "        f\"LINK: {d['link']}\\n\\n\"\n",
    "        f\"ABSTRACT: {d['abstract']}\\n\"\n",
    "    )\n",
    "\n",
    "    pdf_url = abs_to_pdf_url(d[\"id\"])\n",
    "    extra_meta = {\"pdf_url\": pdf_url, \"pdf_pages_indexed\": 0, \"pdf_ok\": False}\n",
    "\n",
    "    try:\n",
    "        pdf_bytes = download_pdf_bytes(pdf_url)\n",
    "        raw = pdf_to_text(pdf_bytes, max_pages=max_pages)\n",
    "        body = strip_references(basic_clean(raw))\n",
    "        extra_meta.update({\"pdf_pages_indexed\": max_pages, \"pdf_ok\": True})\n",
    "        return base + \"\\n\\nFULL_TEXT:\\n\" + body, extra_meta\n",
    "    except Exception as e:\n",
    "         #index abstract-only\n",
    "        extra_meta.update({\"pdf_error\": str(e)})\n",
    "        return base + \"\\n\\nFULL_TEXT: (PDF fetch/parse failed, indexed abstract only)\", extra_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunking\n",
    "def chunk_text(text: str, chunk_size: int = 1400, overlap: int = 250) -> List[str]:\n",
    "    text = \" \".join(text.split())  # normalize whitespace\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "        i += max(1, chunk_size - overlap)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c75565d0f14552ad1597f5e04dbedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dc409b27b0429bb78c856d3dfe6b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f191677d3a6d4f5693e49aec7772a220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e921b320a1a4a6eb19b60ad9592a248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd5d246f6a14aed8f1ac2ad9c5d9376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a324924c26148ad95b214265d82fb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b1f2f8534b4f9e8b70a921e824ec08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9ed124924444b9957504aa467977c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68431105a43484cac6df1c3e7710da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630b88d2dfc44f7aa90b02aab9be49e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533eb6796e7c4c2cbc7e793e773a8972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up Chroma persistent collection with local embeddings\n",
    "# Use Local embedding model for speed\n",
    "emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"arxiv_fullpdf_rag\",\n",
    "    embedding_function=emb_fn\n",
    ")\n",
    "\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'papers': 50,\n",
       " 'pdf_ok': 50,\n",
       " 'pdf_fail': 0,\n",
       " 'chunks_indexed': 1652,\n",
       " 'collection_count': 1652}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 — indexing: fetch PDF -> extract -> clean -> chunk -> upsert into Chroma\n",
    "def index_papers_as_chunks(\n",
    "    collection,\n",
    "    papers: List[Dict],\n",
    "    max_pages: int = 12,\n",
    "    chunk_size: int = 1400,\n",
    "    overlap: int = 250,\n",
    "    throttle_seconds: float = 1.0,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Indexes papers into Chroma.\n",
    "    Returns stats dict.\n",
    "    \"\"\"\n",
    "    ids, texts, metas = [], [], []\n",
    "    pdf_ok = 0\n",
    "    pdf_fail = 0\n",
    "    total_chunks = 0\n",
    "\n",
    "    for p in papers:\n",
    "        full_text, extra = build_fulltext_for_paper(p, max_pages=max_pages)\n",
    "        if extra.get(\"pdf_ok\"):\n",
    "            pdf_ok += 1\n",
    "        else:\n",
    "            pdf_fail += 1\n",
    "\n",
    "        chunks = chunk_text(full_text, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "        for j, ch in enumerate(chunks):\n",
    "            ids.append(f\"{p['id']}::chunk{j}\")\n",
    "            texts.append(ch)\n",
    "            metas.append({\n",
    "                \"paper_id\": p[\"id\"],\n",
    "                \"title\": p[\"title\"],\n",
    "                \"authors\": p[\"authors\"],\n",
    "                \"published\": p[\"published\"],\n",
    "                \"link\": p[\"link\"],\n",
    "                \"chunk\": j,\n",
    "                **extra\n",
    "            })\n",
    "\n",
    "        total_chunks += len(chunks)\n",
    "\n",       
    "        time.sleep(throttle_seconds)\n",
    "\n",
    "    collection.upsert(ids=ids, documents=texts, metadatas=metas)\n",
    "\n",
    "    return {\n",
    "        \"papers\": len(papers),\n",
    "        \"pdf_ok\": pdf_ok,\n",
    "        \"pdf_fail\": pdf_fail,\n",
    "        \"chunks_indexed\": total_chunks,\n",
    "        \"collection_count\": collection.count(),\n",
    "    }\n",
    "\n",
    "stats = index_papers_as_chunks(collection, docs, max_pages=12)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition\n",
      "    Authors: Bo Pan, Peter Zhiping Zhang, Hao-Wei Pang, Alex Zhu, Xiang Yu, Liying Zhang, Liang Zhao\n",
      "    Published: 2026-02-18T18:27:21Z\n",
      "    Link: https://arxiv.org/abs/2602.16684v1\n",
      "    PDF: http://arxiv.org/pdf/2602.16684v1.pdf\n",
      "    Distance: 0.5376\n",
      "    Evidence:\n",
      "    /i Recall/o Database Retrieval 28.57% 57.49% 0.00% REINVENT 4 7.36% 12.21% 1.87% MMPT-FM (Ours) 43.77% 76.45% 11.48% MMPT-RAG (Ours) 46.81% 81.35% 12.99% Table 2: Effect of beam size on average validity of MMPT-FM, averaged on the ChEMBL MMPT held-out test set. Beam 400 600 800 1000 1200 Avg Validity 0.9992 0.9991 0.9989 0.9988 0.9986 (Recall-o = 13.15%). MMPT-RAG further improves performance across all metrics, achieving the highest overall recall (49.21%), the strongest in-training-set recovery (62.08%), and the best outof-training-set recall (15.24%). The gains in Recall indicate that MMPT-RAG effectively helps guide the generator toward a region which is closer to the PMV17 dataset. 5.2.3 Task 3: Cross-Patent Generation (PMV17 →PMV21)Table 1c reports the results for Task 3. Following the same pattern, database retrieval fails entirely on out-of-training transformations, and REINVENT4...\n",
      "\n",
      "[2] CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement\n",
      "    Authors: Kaushal Mhapsekar, Azam Ghanbari, Bita Aslrousta, Samira Mirbagher-Ajorpaz\n",
      "    Published: 2026-02-12T21:28:23Z\n",
      "    Link: https://arxiv.org/abs/2602.12422v1\n",
      "    PDF: http://arxiv.org/pdf/2602.12422v1.pdf\n",
      "    Distance: 0.5383\n",
      "    Evidence:\n",
      "    ate the answer using only those facts. Retrieval can be keyword-based (sparse) or embedding-based (dense) [21]. Some architectures even retrieve during generation to keep long-range context fresh (e.g., RETRO) [4]. Self-RAG variants let the model request or verify evidence mid-answer [1]. In our setting, RAG boils millions of memory-trace entries down to a tiny, verifiable window (e.g., all accesses to a specific PC and address under a given policy) that fits within the LLM’s context. This reduces hallucinations and makes answers checkable. LLMs have limited context windows. To carry information across turns, systems maintain: (i) a sliding buffer of recent messages; (ii) summaries of older turns; and/or (iii) a vector store (embedding index) of past facts that can be re-retrieved when similar questions arise. In effect, this is a conversation memory layer. For trace analysis, buffers ke...\n",
      "\n",
      "[3] LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio\n",
      "    Authors: Naveen Vakada, Kartik Hegde, Arvind Krishna Sridhar, Yinyi Guo, Erik Visser\n",
      "    Published: 2026-02-16T10:15:22Z\n",
      "    Link: https://arxiv.org/abs/2602.14612v1\n",
      "    PDF: http://arxiv.org/pdf/2602.14612v1.pdf\n",
      "    Distance: 0.5437\n",
      "    Evidence:\n",
      "    ; Goel et al., 2025). Despite these advances, long audio QA remains difficult in practice. First, raw multi-hour audio cannot be directly ingested by most models due to context-length and computation constraints, encouraging approaches that compress, segment, or selectively attend to relevant evidence. Second, natural-language time expressions are highly variable (12-hour vs. 24-hour time, shift references, relative phrases such as ‘before 5pm’), and errors in time interpretation can invalidate downstream reasoning. Third, open-ended generation over long logs is prone to hallucination unless responses are explicitly grounded in verifiable evidence. Retrieval-augmented generation (RAG) reduces hallucinations by grounding model outputs in retrieved evidence (Lewis et al., 2020; Gupta et al., 2024). Although widely used for knowledgeintensive QA and summarization, RAG performance is tightly...\n",
      "\n",
      "[4] GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\n",
      "    Authors: Zhan Qu, Michael Färber\n",
      "    Published: 2026-02-13T11:30:37Z\n",
      "    Link: https://arxiv.org/abs/2602.12828v1\n",
      "    PDF: http://arxiv.org/pdf/2602.12828v1.pdf\n",
      "    Distance: 0.5449\n",
      "    Evidence:\n",
      "    ction performance across diagnoses, procedures, medications, and laboratory tests. Beyond raw performance gains, the results reveal several consistent behaviors that help clarify when and why structured geometric retrieval is effective for longitudinal EHR modeling. Retrieval as a robustness mechanism under clinical noise. A first observation is that all retrieval-based approaches outperform sequence-only baselines across modalities. This is particularly evident for procedures, medications, and labs, where unordered code sets, sparsity, and institutionspecific coding practices introduce substantial noise. Restricting prediction to a candidate set grounded in historical co-occurrence and hierarchy appears to act as an implicit regularizer, reducing the burden on the model to discriminate among thousands of rarely observed codes. The consistent improvement of Euclidean RAG over BEHRT and T...\n",
      "\n",
      "[5] Evolutionary Context Search for Automated Skill Acquisition\n",
      "    Authors: Qi Sun, Stefan Nielsen, Rio Yokota, Yujin Tang\n",
      "    Published: 2026-02-18T00:47:02Z\n",
      "    Link: https://arxiv.org/abs/2602.16113v1\n",
      "    PDF: http://arxiv.org/pdf/2602.16113v1.pdf\n",
      "    Distance: 0.5768\n",
      "    Evidence:\n",
      "    ncur substantial computational costs while struggling to obtain the required skill. Training-based methods, such as supervised finetuning (SFT) and reinforcement learning (RL) on curated data, are expensive due to their computational requirements, with additional engineering costs incurred by data collection and processing (Cottier et al., 2024). Moreover, given posttraining requires weight access, such methods are naturally inapplicable to frontier, closed-source models. Current incontext approaches offer only partial solutions to trainingbased methods. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Ram et al., 2023; Khandelwal et al., 2019) can equip base models with new knowledge at testtime without necessitating weight access, but similaritybased retrieval often fails because queries tend to be verbose, contain irrelevant context, or not task-specific (Li et al., 2023a; Pe...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def retrieve(\n",
    "    collection,\n",
    "    query: str,\n",
    "    k: int = 5\n",
    ") -> List[Tuple[str, Dict, float]]:\n",
    "    res = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    docs_ = res[\"documents\"][0]\n",
    "    metas_ = res[\"metadatas\"][0]\n",
    "    dists_ = res[\"distances\"][0]\n",
    "    return list(zip(docs_, metas_, dists_))\n",
    "\n",
    "def show_hits(hits: List[Tuple[str, Dict, float]], max_chars: int = 900):\n",
    "    for i, (txt, meta, dist) in enumerate(hits, start=1):\n",
    "        print(f\"\\n[{i}] {meta['title']}\")\n",
    "        print(f\"    Authors: {meta['authors']}\")\n",
    "        print(f\"    Published: {meta['published']}\")\n",
    "        print(f\"    Link: {meta['link']}\")\n",
    "        if meta.get(\"pdf_url\"):\n",
    "            print(f\"    PDF: {meta['pdf_url']}\")\n",
    "        print(f\"    Distance: {dist:.4f}\")\n",
    "        print(\"    Evidence:\")\n",
    "        snippet = txt[:max_chars] + (\"...\" if len(txt) > max_chars else \"\")\n",
    "        print(\"    \" + snippet)\n",
    "\n",
    "q = \"What retrieval strategies are commonly used in RAG systems, and why?\"\n",
    "hits = retrieve(collection, q, k=5)\n",
    "show_hits(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Insert Key Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_for_llm(hits, max_chars_per_chunk=1200):\n",
    "\n",
    "    blocks = []\n",
    "    sources_meta = []\n",
    "    for i, (txt, meta, dist) in enumerate(hits, start=1):\n",
    "        snippet = txt[:max_chars_per_chunk]\n",
    "        blocks.append(\n",
    "            f\"[{i}] TITLE: {meta.get('title','')}\\n\"\n",
    "            f\"AUTHORS: {meta.get('authors','')}\\n\"\n",
    "            f\"PUBLISHED: {meta.get('published','')}\\n\"\n",
    "            f\"LINK: {meta.get('link','')}\\n\"\n",
    "            f\"PDF: {meta.get('pdf_url','')}\\n\"\n",
    "            f\"EXCERPT:\\n{snippet}\\n\"\n",
    "        )\n",
    "        sources_meta.append(meta)\n",
    "    return \"\\n\\n\".join(blocks), sources_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "SYSTEM_INSTRUCTIONS = \"\"\"You are a research assistant.\n",
    "Answer the user's question using ONLY the provided sources.\n",
    "If the sources do not contain the answer, say: \"I don't know based on the provided documents.\"\n",
    "When you use information from a source, cite it inline like [1] or [2].\n",
    "Do not cite sources that you did not use.\n",
    "Keep the answer concise and structured (bullets are fine).\"\"\"\n",
    "\n",
    "def generate_rag_answer(question: str, hits, model: str = \"gpt-4.1-mini\"):\n",
    "    context_text, sources_meta = build_context_for_llm(hits)\n",
    "\n",
    "    user_message = f\"\"\"QUESTION:\n",
    "{question}\n",
    "\n",
    "SOURCES:\n",
    "{context_text}\n",
    "\"\"\"\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=model,\n",
    "        instructions=SYSTEM_INSTRUCTIONS,\n",
    "        input=user_message,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    answer_text = resp.output_text\n",
    "    return answer_text, sources_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common retrieval strategies used in Retrieval-Augmented Generation (RAG) systems include:\n",
      "\n",
      "- **Keyword-based (sparse) retrieval:** This method retrieves documents or evidence based on matching keywords from the query. It is straightforward but may miss semantically relevant information if exact keywords are not present.\n",
      "\n",
      "- **Embedding-based (dense) retrieval:** This approach uses vector embeddings to represent queries and documents, retrieving items based on semantic similarity rather than exact keyword matches. It is more flexible and effective for capturing nuanced meanings [2].\n",
      "\n",
      "- **Retrieval during generation:** Some architectures, such as RETRO, retrieve relevant information dynamically during the generation process to keep the model's context fresh and reduce hallucinations [2].\n",
      "\n",
      "- **Self-RAG variants:** These allow the model to request or verify evidence mid-answer, improving the grounding and accuracy of generated responses [2].\n",
      "\n",
      "- **Structured geometric retrieval:** In specific domains like longitudinal electronic health records (EHR), retrieval based on structured embeddings (e.g., hyperbolic geometry) helps by restricting predictions to candidate sets grounded in historical co-occurrence and hierarchy, acting as an implicit regularizer and improving robustness against noise [4].\n",
      "\n",
      "These strategies are used because:\n",
      "\n",
      "- They help reduce hallucinations by grounding model outputs in retrieved, verifiable evidence [3].\n",
      "\n",
      "- They manage the limited context window of large language models by maintaining a memory layer of relevant past facts or evidence [2].\n",
      "\n",
      "- They improve recall and guide generation towards more relevant or in-distribution outputs, as seen in molecular transformation tasks [1].\n",
      "\n",
      "- They provide robustness against noise and sparsity in data by restricting candidate predictions to plausible sets [4].\n",
      "\n",
      "- They enable test-time knowledge augmentation without requiring expensive retraining or weight access [5].\n",
      "\n",
      "In summary, RAG systems commonly use a combination of sparse and dense retrieval methods, dynamic retrieval during generation, and domain-specific structured retrieval to improve grounding, reduce hallucinations, and enhance performance across tasks [1][2][3][4][5].\n"
     ]
    }
   ],
   "source": [
    "question = \"What retrieval strategies are commonly used in RAG systems, and why?\"\n",
    "hits = retrieve(collection, question, k=5) \n",
    "\n",
    "answer, sources_meta = generate_rag_answer(question, hits)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CITATIONS:\n",
      "[1] Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition\n",
      "    https://arxiv.org/abs/2602.16684v1\n",
      "    PDF: http://arxiv.org/pdf/2602.16684v1.pdf\n",
      "\n",
      "[2] CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement\n",
      "    https://arxiv.org/abs/2602.12422v1\n",
      "    PDF: http://arxiv.org/pdf/2602.12422v1.pdf\n",
      "\n",
      "[3] LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio\n",
      "    https://arxiv.org/abs/2602.14612v1\n",
      "    PDF: http://arxiv.org/pdf/2602.14612v1.pdf\n",
      "\n",
      "[4] GRAIL: Geometry-Aware Retrieval-Augmented Inference with LLMs over Hyperbolic Representations of Patient Trajectories\n",
      "    https://arxiv.org/abs/2602.12828v1\n",
      "    PDF: http://arxiv.org/pdf/2602.12828v1.pdf\n",
      "\n",
      "[5] Evolutionary Context Search for Automated Skill Acquisition\n",
      "    https://arxiv.org/abs/2602.16113v1\n",
      "    PDF: http://arxiv.org/pdf/2602.16113v1.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_citations(sources_meta):\n",
    "    print(\"\\nCITATIONS:\")\n",
    "    for i, m in enumerate(sources_meta, start=1):\n",
    "        print(f\"[{i}] {m.get('title','')}\")\n",
    "        print(f\"    {m.get('link','')}\")\n",
    "        if m.get(\"pdf_url\"):\n",
    "            print(f\"    PDF: {m.get('pdf_url')}\")\n",
    "        print()\n",
    "\n",
    "print_citations(sources_meta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag-arxiv)",
   "language": "python",
   "name": "rag-arxiv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
